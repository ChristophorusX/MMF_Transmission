# A sparsity regularized model for the fiber-transimitted image dataset.
from typing import Any, Callable, List, Optional, Union

import cvnn
import numpy as np
import tensorflow as tf
from cvnn.layers import ComplexDense, ComplexInput
from tensorflow.keras import Model, backend, layers
from tensorflow.keras.losses import Loss
from tensorflow.keras.optimizers import Adam

t_input = Union[tf.Tensor, tuple, list]
t_input_shape = Union[tf.TensorShape, List[tf.TensorShape]]

DEFAULT_COMPLEX_TYPE = tf.as_dtype(np.complex64)

# From CVNN package
def complex_input(shape=None, batch_size=None, name=None, dtype=DEFAULT_COMPLEX_TYPE,
                  sparse=False, tensor=None, ragged=False, **kwargs):
    """
    `complex_input()` is used to instantiate a Keras tensor.
    A Keras tensor is a TensorFlow symbolic tensor object,
    which we augment with certain attributes that allow us to build a Keras model
    just by knowing the inputs and outputs of the model.
    For instance, if `a`, `b` and `c` are Keras tensors,
    it becomes possible to do:
    `model = Model(input=[a, b], output=c)`
    Arguments:
      shape: A shape tuple (integers), not including the batch size.
          For instance, `shape=(32,)` indicates that the expected input
          will be batches of 32-dimensional vectors. Elements of this tuple
          can be None; 'None' elements represent dimensions where the shape is
          not known.
      batch_size: optional static batch size (integer).
      name: An optional name string for the layer.
          Should be unique in a model (do not reuse the same name twice).
          It will be autogenerated if it isn't provided.
      dtype: The data type expected by the input
      sparse: A boolean specifying whether the placeholder to be created is
          sparse. Only one of 'ragged' and 'sparse' can be True. Note that,
          if `sparse` is False, sparse tensors can still be passed into the
          input - they will be densified with a default value of 0.
      tensor: Optional existing tensor to wrap into the `Input` layer.
          If set, the layer will use the `tf.TypeSpec` of this tensor rather
          than creating a new placeholder tensor.
      ragged: A boolean specifying whether the placeholder to be created is
          ragged. Only one of 'ragged' and 'sparse' can be True. In this case,
          values of 'None' in the 'shape' argument represent ragged dimensions.
          For more information about RaggedTensors, see
          [this guide](https://www.tensorflow.org/guide/ragged_tensors).
      **kwargs: deprecated arguments support. Supports `batch_shape` and
          `batch_input_shape`.
    Returns:
        A `tensor`.
    Example:
    ```python
        # this is a logistic regression in Keras
        x = complex_input(shape=(32,))
        y = Dense(16, activation='softmax')(x)
        model = Model(x, y)
    ```
    Note that even if eager execution is enabled,
    `Input` produces a symbolic tensor (i.e. a placeholder).
    This symbolic tensor can be used with other
    TensorFlow ops, as such:
    ```python
        x = complex_input(shape=(32,))
        y = tf.square(x)
    ```
    Raises:
        ValueError: If both `sparse` and `ragged` are provided.
        ValueError: If both `shape` and (`batch_input_shape` or `batch_shape`) are provided.
        ValueError: If both `shape` and `tensor` are None.
        ValueError: if any unrecognized parameters are provided.
    """
    if sparse and ragged:
        raise ValueError(
            'Cannot set both sparse and ragged to True in a Keras input.')

    dtype = tf.as_dtype(dtype)
    input_layer_config = {'name': name, 'dtype': dtype.name, 'sparse': sparse,
                          'ragged': ragged, 'input_tensor': tensor}

    batch_input_shape = kwargs.pop('batch_input_shape',
                                   kwargs.pop('batch_shape', None))
    if shape is not None and batch_input_shape is not None:
        raise ValueError('Only provide the `shape` OR `batch_input_shape` argument '
                         'to Input, not both at the same time.')
    if batch_input_shape is None and shape is None and tensor is None:
        raise ValueError('Please provide to Input either a `shape`'
                         ' or a `tensor` argument. Note that '
                         '`shape` does not include the batch '
                         'dimension.')
    if kwargs:
        raise ValueError('Unrecognized keyword arguments:', kwargs.keys())

    if batch_input_shape:
        shape = batch_input_shape[1:]
        input_layer_config.update({'batch_input_shape': batch_input_shape})
    else:
        input_layer_config.update(
            {'batch_size': batch_size, 'input_shape': shape})
    # import pdb; pdb.set_trace()
    input_layer = ComplexInput(**input_layer_config)

    # Return tensor including `_keras_history`.
    # Note that in this case train_output and test_output are the same pointer.
    outputs = input_layer._inbound_nodes[0].output_tensors
    if isinstance(outputs, list) and len(outputs) == 1:
        return outputs[0]
    else:
        return outputs

# From CVNN package
class ComplexMeanSquareError(Loss):

    def call(self, y_true, y_pred):
        y_pred = tf.convert_to_tensor(y_pred)
        y_true = tf.convert_to_tensor(y_true)
        if y_pred.dtype.is_complex and not y_true.dtype.is_complex:     # Complex pred but real true
            y_true = tf.complex(y_true, y_true)
        y_true = tf.cast(y_true, y_pred.dtype)
        return tf.cast(backend.mean(tf.math.square(tf.math.abs(y_true - y_pred)), axis=-1),
                       dtype=y_pred.dtype.real_dtype)


class TransmissionMatrixGenerator(layers.Layer):
    """A custom layer that generates the transmission matrix from fiber configuration features.

    """

    def __init__(
        self,
        num_fiber_features: int,
        num_invariant_fields: int,
        **kwargs,
    ):
        """Initialize the transmission matrix generator.

        Args:
            num_fiber_features (int): The number of fiber configuration features.
            num_invariant_fields (int): The number of invariant fields.
        """
        super(TransmissionMatrixGenerator, self).__init__(**kwargs)
        self.num_fiber_features = num_fiber_features
        self.num_invariant_fields = num_invariant_fields
        self.dense1 = ComplexDense(
            2 * self.num_fiber_features,
            activation="cart_relu",
        )
        self.dense2 = ComplexDense(
            2 * self.num_fiber_features,
            activation="cart_relu",
        )
        self.dense3 = ComplexDense(
            self.num_invariant_fields ** 2,
            activation="cart_relu",
        )
        self.reshape = layers.Reshape(
            (self.num_invariant_fields, self.num_invariant_fields))

    def call(self, inputs: tf.Tensor) -> tf.Tensor:
        """Forward pass of the transmission matrix generator."""

        x = self.dense1(inputs)
        x = self.dense2(x)
        x = self.dense3(x)
        x = self.reshape(x)
        return x


def construct_model(
    sample_dimension: int,
    fiber_features: int,
    activation: Union[str, Callable] = "cart_relu",
    regularization: float = 0.0,
) -> Model:
    """Constructs the Sparsity-Regularized model.

    Args:
        sample_dimension (int): The input dimension of samples.
        fiber_features (int): The number of fiber configuration features.
        activation (Union[str, Callable], optional): The activation function. 
            Defaults to "cart_relu".
        regularization (float, optional): The regularization factor. Defaults to 0.0.

    Returns:
        Model: A sparsity-regularized model.
    """
    sample_inputs = complex_input(
        shape=(sample_dimension,),
        name="sample_inputs",
    )
    fiber_features = complex_input(
        shape=(fiber_features,),
        name="fiber_features",
    )

    x = sample_inputs
    h = ComplexDense(sample_dimension, activation=None)(x)
    transmission_matrix = TransmissionMatrixGenerator(
        sample_dimension, sample_dimension)(fiber_features)
    g = tf.einsum('bi,bij->bj', h, transmission_matrix)
    inversed = ComplexDense(sample_dimension, activation=None)
    outputs = inversed(g)
    inversed_h = inversed(h)

    return Model(
        inputs=[sample_inputs, fiber_features],
        outputs=tf.stack([outputs, inversed_h], axis=-1),
        name="sparsity_regularized model",
    )


class ModelLoss(Loss):
    def call(
        self,
        y_true: tf.Tensor,
        y_pred: tf.Tensor,
    ) -> tf.Tensor:
        """The loss function of the sparsity-regularized model.

        Args:
            y_true (tf.Tensor): The true value.
            y_pred (tf.Tensor): The predicted value.

        Returns:
            tf.Tensor: The loss value.
        """
        labels = y_true[..., 0]
        outputs = y_pred[..., 0]
        inputs = y_true[..., 1]
        inversed_h = y_pred[..., 1]

        return ComplexMeanSquareError()(labels, outputs) + 0.1 * ComplexMeanSquareError()(inputs, inversed_h)


def configure_model(
    model: Model,
    regularization: float,
    optimizer: Any = None,
    metrics: Optional[List[Union[Callable, str]]] = None,
    learning_rate: float = 1e-4,
):
    """Configures the sparsity-regularized model.

    Args:
        model (Model): The model to be configured.
        regularization (float): The regularization coefficient.
        optimizer (Any, optional): The optimizer. Defaults to None.
        metrics (Optional[List[Union[Callable, str]]], optional): The metrics for tracking. 
            Defaults to None.
        learning_rate (float, optional): The learning rate. Defaults to 1e-4.
    """

    if optimizer is None:
        optimizer = Adam(learning_rate=learning_rate)

    if metrics is None:
        metrics = [
            'mse',
        ]

    model.compile(
        loss=ModelLoss(),
        optimizer=optimizer,
        metrics=metrics,
        run_eagerly=True,
    )


if __name__ == '__main__':
    sample_inputs = tf.cast(tf.random.normal((320, 10)), dtype=tf.complex64)
    fiber_features = tf.cast(tf.random.normal((320, 5)), dtype=tf.complex64)
    labels = tf.cast(tf.random.normal((320, 10)), dtype=tf.complex64)
    model = construct_model(sample_dimension=10, fiber_features=5)
    configure_model(model, regularization=0.1)
    print(model.summary())
    # out = model([sample_inputs, fiber_features])
    # print(out.shape)
    
    history = model.fit(
        [sample_inputs, fiber_features],
        tf.stack([labels, sample_inputs], axis=-1),
        batch_size=32,
        epochs=100,
    )
